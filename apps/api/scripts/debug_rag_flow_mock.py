"""
Script debug vá»›i MOCK DATA - khÃ´ng cáº§n ChromaDB.
"""
import asyncio
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from app.rag.llm import LLMWrapper
from app.core.config import settings


def print_section(title: str):
    """In tiÃªu Ä‘á» section vá»›i format Ä‘áº¹p."""
    print("\n" + "=" * 80)
    print(f"  {title}")
    print("=" * 80)


def print_subsection(title: str):
    """In tiÃªu Ä‘á» subsection."""
    print(f"\n--- {title} ---")


async def debug_rag_flow_mock(question: str):
    """
    Debug flow RAG vá»›i mock data - khÃ´ng cáº§n ChromaDB.
    Hiá»ƒn thá»‹ cÃ¡ch Gemini sáº¯p xáº¿p document chunks thÃ nh cÃ¢u tráº£ lá»i.
    """
    print_section("ğŸ” DEBUG RAG FLOW - MOCK DATA (KhÃ´ng cáº§n ChromaDB)")

    # 1. Creat LLM Wrapper
    print_subsection("1. Khá»Ÿi táº¡o LLM Wrapper")
    llm_wrapper = LLMWrapper()

    print(f"   âœ“ LLM Wrapper: {type(llm_wrapper).__name__}")
    print(f"   âœ“ LLM Model: {settings.LLM_MODEL}")
    print(f"   âœ“ Temperature: {settings.LLM_TEMPERATURE}")
    print(f"   âœ“ Max Tokens: {settings.LLM_MAX_TOKENS}")
    print(f"   âœ“ Max Context Chars: {llm_wrapper.max_context_chars}")

    # 2. Mock document contexts
    print_section("2. MOCK DOCUMENT CONTEXTS (Giáº£ láº­p documents tá»« vector store)")
    print(f"   CÃ¢u há»i: '{question}'")

    # Mock contexts
    mock_contexts = [
        {
            "text": "Thá»i háº¡n ná»™p há»c phÃ­ ká»³ há»c chÃ­nh nÄƒm 2024 lÃ  tá»« ngÃ y 15/08/2024 Ä‘áº¿n háº¿t ngÃ y 30/08/2024. Sinh viÃªn cáº§n ná»™p há»c phÃ­ táº¡i phÃ²ng TÃ i chÃ­nh Káº¿ toÃ¡n hoáº·c chuyá»ƒn khoáº£n qua ngÃ¢n hÃ ng. Sá»‘ tÃ i khoáº£n: 1234567890 - NgÃ¢n hÃ ng ABC. LiÃªn há»‡: tcketoan@greenwich.edu.vn hoáº·c sá»‘ Ä‘iá»‡n thoáº¡i 024-xxxx-xxxx.",
            "metadata": {
                "source": "handbook_2024.pdf",
                "page": 15,
                "document_id": "doc-001",
                "chunk_id": "chunk-001",
                "chunk_index": 0,
            },
            "score": 0.92,
        },
        {
            "text": "Quy Ä‘á»‹nh vá» há»c phÃ­: Sinh viÃªn pháº£i hoÃ n thÃ nh viá»‡c ná»™p há»c phÃ­ trÆ°á»›c ngÃ y báº¯t Ä‘áº§u ká»³ há»c. Náº¿u quÃ¡ háº¡n, sinh viÃªn sáº½ bá»‹ cáº£nh bÃ¡o vÃ  cÃ³ thá»ƒ bá»‹ Ä‘Ã¬nh chá»‰ há»c táº­p. Má»©c há»c phÃ­ Ä‘Æ°á»£c quy Ä‘á»‹nh theo tá»«ng chÆ°Æ¡ng trÃ¬nh Ä‘Ã o táº¡o.",
            "metadata": {
                "source": "enrollment_guide.pdf",
                "page": 5,
                "document_id": "doc-002",
                "chunk_id": "chunk-002",
                "chunk_index": 0,
            },
            "score": 0.88,
        },
        {
            "text": "Há»c bá»•ng vÃ  há»— trá»£ tÃ i chÃ­nh: Sinh viÃªn cÃ³ thá»ƒ Ä‘Äƒng kÃ½ há»c bá»•ng náº¿u Ä‘Ã¡p á»©ng cÃ¡c Ä‘iá»u kiá»‡n vá» há»c lá»±c vÃ  hoÃ n cáº£nh gia Ä‘Ã¬nh. Thá»i háº¡n Ä‘Äƒng kÃ½ há»c bá»•ng lÃ  tá»« 01/08 Ä‘áº¿n 15/08 hÃ ng nÄƒm.",
            "metadata": {
                "source": "scholarship_info.pdf",
                "page": 2,
                "document_id": "doc-003",
                "chunk_id": "chunk-003",
                "chunk_index": 0,
            },
            "score": 0.75,
        },
    ]

    print(f"\n   âœ… Mock {len(mock_contexts)} document chunks:")
    for i, ctx in enumerate(mock_contexts, start=1):
        score = ctx.get("score", 0.0)
        source = ctx.get("metadata", {}).get("source", "N/A")
        page = ctx.get("metadata", {}).get("page")
        text_preview = ctx.get("text", "")[:100] + "..." if len(ctx.get("text", "")) > 100 else ctx.get("text", "")

        print(f"\n   ğŸ“„ Chunk {i}:")
        print(f"      â€¢ Score: {score:.4f}")
        print(f"      â€¢ Source: {source}")
        if page:
            print(f"      â€¢ Page: {page}")
        print(f"      â€¢ Content: {text_preview}")
        print(f"      â€¢ Full length: {len(ctx.get('text', ''))} kÃ½ tá»±")

    # 3. Format contexts thÃ nh prompt
    print_section("3. FORMAT CONTEXT (Sáº¯p xáº¿p document chunks thÃ nh prompt)")
    context_text = llm_wrapper.format_contexts(mock_contexts, max_sources=8)

    print(f"   Context length: {len(context_text)} kÃ½ tá»±")
    print(f"   Max context chars: {llm_wrapper.max_context_chars}")
    print(f"\n   ğŸ“ CONTEXT TEXT (sáº½ gá»­i Ä‘áº¿n Gemini):")
    print("   " + "-" * 76)

    for line in context_text.split("\n"):
        print(f"   {line}")
    print("   " + "-" * 76)

    # 4. Prompt template
    print_section("4. PROMPT TEMPLATE (Máº«u prompt gá»­i Ä‘áº¿n Gemini)")
    print("   System Prompt:")
    for line in llm_wrapper.system_prompt.split("\n"):
        print(f"      {line}")

    print("\n   Prompt Structure:")
    print("      [System] Báº¡n lÃ  trá»£ lÃ½ AI... (rules)")
    print("      [System] Context:\n{context}")
    print("      [Human] {question}")

    # 5. create prompt
    print_section("5. PROMPT HOÃ€N CHá»ˆNH (Prompt cuá»‘i cÃ¹ng gá»­i Ä‘áº¿n Gemini)")
    # Format prompt
    messages = llm_wrapper.prompt.format_messages(
        context=context_text,
        question=question
    )

    print("   Messages structure:")
    for i, msg in enumerate(messages, start=1):
        msg_type = msg.__class__.__name__
        content = msg.content

        print(f"\n   Message {i} ({msg_type}):")
        print("      " + "-" * 72)
        for line in content.split("\n"):
            if len(line) > 200:
                print(f"      {line[:200]}...")
            else:
                print(f"      {line}")
        print("      " + "-" * 72)

    # 6. Call Gemini API
    print_section("6. Gá»ŒI GEMINI API (Generate answer)")
    print("   â³ Äang gá»i Gemini API...")

    try:
        answer = await llm_wrapper.generate_answer_async(question, mock_contexts)
        print(f"   âœ… ThÃ nh cÃ´ng!")
        print(f"\n   ğŸ“¤ Response tá»« Gemini:")
        print("   " + "-" * 76)
        # In tá»«ng dÃ²ng vá»›i indent
        for line in answer.split("\n"):
            print(f"   {line}")
        print("   " + "-" * 76)
    except Exception as e:
        print(f"   âŒ Lá»—i: {e}")
        import traceback
        traceback.print_exc()
        return

    # 7. Gemini sort
    print_section("7. PHÃ‚N TÃCH CÃCH GEMINI Sáº®P Xáº¾P DOCUMENT")
    print("   âœ… Gemini Ä‘Ã£ nháº­n Ä‘Æ°á»£c cÃ¡c document chunks theo thá»© tá»±:")
    for i, ctx in enumerate(mock_contexts, start=1):
        source = ctx.get("metadata", {}).get("source", "N/A")
        print(f"      {i}. {source} (score: {ctx.get('score', 0):.4f})")

    print("\n   âœ… Gemini Ä‘Ã£ format context nhÆ° sau:")
    print("      - GhÃ©p cÃ¡c chunks theo format: [Nguá»“n X - source.pdf (trang Y)]\\ncontent")
    print("      - Sáº¯p xáº¿p theo thá»© tá»± score tá»« cao xuá»‘ng tháº¥p")
    print("      - Clip náº¿u vÆ°á»£t quÃ¡ max_context_chars")

    print("\n   âœ… Gemini Ä‘Ã£ táº¡o cÃ¢u tráº£ lá»i:")
    print("      - Dá»±a trÃªn context Ä‘Æ°á»£c cung cáº¥p")
    print("      - Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t")
    print("      - Tá»•ng há»£p thÃ´ng tin tá»« nhiá»u nguá»“n")
    print("      - Giá»¯ nguyÃªn cÃ¡c thÃ´ng tin chi tiáº¿t (ngÃ y thÃ¡ng, email, sá»‘ Ä‘iá»‡n thoáº¡i)")

    print_section("âœ… HOÃ€N Táº¤T")


if __name__ == "__main__":
    # Set UTF-8 encoding for Windows console
    if sys.platform == "win32":
        import io
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')


    question = "Há»c phÃ­ Ä‘áº¡i há»c Greenwich Viá»‡t Nam lÃ  bao nhiÃªu?"

    # if have argument at command line, use:
    if len(sys.argv) > 1:
        question = " ".join(sys.argv[1:])

    print(f"\nğŸš€ Báº¯t Ä‘áº§u debug RAG flow (MOCK DATA) vá»›i cÃ¢u há»i: '{question}'\n")
    print("â„¹ï¸  Script nÃ y KHÃ”NG cáº§n ChromaDB - chá»‰ test vá»›i Gemini API\n")

    asyncio.run(debug_rag_flow_mock(question))

