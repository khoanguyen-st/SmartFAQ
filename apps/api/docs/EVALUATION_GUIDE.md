# RAG Evaluation Framework

## Overview

Framework ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng RAG pipeline v·ªõi 2 nh√≥m metrics:
- **Retrieval Metrics**: Precision@K, Recall@K, F1@K, MRR, NDCG, Hit Rate
- **Answer Quality Metrics**: Confidence, Fallback Rate, Latency, Number of Sources

## Quick Start

### 1. T·∫°o Test Cases

T·∫°o file `tests/evaluation/test_cases.json`:

```json
[
  {
    "question": "H·ªçc ph√≠ nƒÉm h·ªçc 2024 l√† bao nhi√™u?",
    "relevant_doc_ids": ["tuition_2024"],
    "department_id": 1,
    "language": "vi"
  },
  {
    "question": "CNTT",
    "relevant_doc_ids": ["program_cntt"],
    "department_id": 2,
    "language": "vi"
  }
]
```

### 2. Run Evaluation

```bash
cd apps/api
python scripts/run_evaluation.py
```

### 3. Check Results

K·∫øt qu·∫£ ƒë∆∞·ª£c l∆∞u v√†o `tests/evaluation/results.json` v√† in ra console:

```
======================================================================
RAG EVALUATION REPORT
======================================================================

Total Test Cases: 10
Top-K: 5

üìä RETRIEVAL METRICS:
----------------------------------------------------------------------
  Precision@5: 0.650
  Recall@5:    0.820
  F1@5:         0.725
  MRR:            0.780
  NDCG:           0.845
  Hit Rate:       95.00%

ü§ñ ANSWER QUALITY METRICS:
----------------------------------------------------------------------
  Avg Confidence: 0.742
  Fallback Rate:  8.00%
  Avg Latency:    1250ms
  Avg Sources:    4.2

======================================================================
```

## Metrics Explained

### Retrieval Metrics

**Precision@K**: T·ªâ l·ªá k·∫øt qu·∫£ relevant trong top-K
- `Precision@5 = 0.650` ‚Üí 65% k·∫øt qu·∫£ trong top-5 l√† relevant

**Recall@K**: T·ªâ l·ªá relevant documents ƒë∆∞·ª£c t√¨m th·∫•y trong top-K
- `Recall@5 = 0.820` ‚Üí T√¨m ƒë∆∞·ª£c 82% relevant documents

**F1@K**: Harmonic mean c·ªßa Precision v√† Recall
- `F1@5 = 0.725` ‚Üí Balance gi·ªØa precision v√† recall

**MRR (Mean Reciprocal Rank)**: Trung b√¨nh ngh·ªãch ƒë·∫£o c·ªßa rank ƒë·∫ßu ti√™n c√≥ relevant result
- `MRR = 0.780` ‚Üí Trung b√¨nh relevant result ƒë·∫ßu ti√™n ·ªü v·ªã tr√≠ 1.28

**NDCG (Normalized Discounted Cumulative Gain)**: ƒê√°nh gi√° ranking quality
- `NDCG = 0.845` ‚Üí Ranking quality cao (1.0 l√† perfect)

**Hit Rate**: % queries c√≥ √≠t nh·∫•t 1 relevant result trong top-K
- `Hit Rate = 95%` ‚Üí 95% queries t√¨m ƒë∆∞·ª£c √≠t nh·∫•t 1 relevant result

### Answer Quality Metrics

**Avg Confidence**: Trung b√¨nh confidence score c·ªßa answers
- `0.742` ‚Üí H·ªá th·ªëng kh√° t·ª± tin v·ªÅ c√¢u tr·∫£ l·ªùi

**Fallback Rate**: T·ªâ l·ªá queries trigger fallback
- `8%` ‚Üí 8% queries kh√¥ng ƒë·ªß context ƒë·ªÉ tr·∫£ l·ªùi

**Avg Latency**: Th·ªùi gian trung b√¨nh ƒë·ªÉ generate answer
- `1250ms` ‚Üí Trung b√¨nh 1.25 gi√¢y/query

**Avg Sources**: S·ªë l∆∞·ª£ng sources trung b√¨nh ƒë∆∞·ª£c s·ª≠ d·ª•ng
- `4.2` ‚Üí Trung b√¨nh s·ª≠ d·ª•ng 4.2 sources/answer

## Test Cases Format

### Basic Format

```json
{
  "question": "C√¢u h·ªèi test",
  "relevant_doc_ids": ["doc_id_1", "doc_id_2"],
  "department_id": 1,
  "language": "vi"
}
```

### Advanced Format (v·ªõi chunk-level evaluation)

```json
{
  "question": "H·ªçc ph√≠ l√† bao nhi√™u?",
  "relevant_doc_ids": ["tuition_2024"],
  "relevant_chunk_ids": ["chunk_abc", "chunk_def"],
  "expected_answer": "H·ªçc ph√≠ nƒÉm 2024 l√† 25 tri·ªáu ƒë·ªìng/nƒÉm",
  "department_id": 1,
  "language": "vi"
}
```

### Fields

- `question` (required): C√¢u h·ªèi test
- `relevant_doc_ids` (required): IDs c·ªßa documents relevant
- `relevant_chunk_ids` (optional): IDs c·ªßa chunks relevant (chi ti·∫øt h∆°n doc-level)
- `expected_answer` (optional): C√¢u tr·∫£ l·ªùi mong ƒë·ª£i (ƒë·ªÉ ƒë√°nh gi√° answer quality)
- `department_id` (optional nh∆∞ng n√™n c√≥): Ph√≤ng ban s·ªü h·ªØu t√†i li·ªáu (kh·ªõp `documents.department_id`)
- `category` (optional): Tag ƒë·ªÉ ph√¢n nh√≥m/ƒë√°nh gi√° coverage (kh√¥ng ƒë∆∞·ª£c l∆∞u trong DB)
- `language` (optional): Ng√¥n ng·ªØ (vi, en)

## How to Get Document/Chunk IDs

### Option 1: From Database

```python
from app.repositories.faq_repository import FAQRepository

repo = FAQRepository()
faqs = await repo.get_all()

for faq in faqs:
    print(f"Question: {faq.question}")
    print(f"Doc ID: {faq.id}")
```

### Option 2: From ChromaDB

```python
from app.rag.retriever import Retriever

retriever = Retriever()
results = retriever.retrieve("h·ªçc ph√≠", top_k=5)

for result in results:
    print(f"Chunk ID: {result.get('chunk_id')}")
    print(f"Doc ID: {result.get('document_id')}")
    print(f"Content: {result.get('content')[:100]}...")
```

### Option 3: Manual Labeling

1. Query system v·ªõi test question
2. Xem sources ƒë∆∞·ª£c tr·∫£ v·ªÅ
3. Note l·∫°i IDs c·ªßa sources relevant
4. Add v√†o test cases

## Best Practices

### 1. Diverse Test Cases

Cover nhi·ªÅu ph√≤ng ban:
```json
[
  {"department_id": 1, "question": "H·ªçc ph√≠..."},
  {"department_id": 2, "question": "CNTT..."},
  {"department_id": 3, "question": "Th√¥i h·ªçc..."},
  {"department_id": 5, "question": "H·ªçc b·ªïng..."}
]
```

### 2. Query Complexity Levels

- **Short queries**: "CNTT", "H·ªçc ph√≠"
- **Medium queries**: "H·ªçc ph√≠ nƒÉm 2024"
- **Long queries**: "L√†m th·∫ø n√†o ƒë·ªÉ ƒëƒÉng k√Ω h·ªçc l·∫°i m√¥n b·ªã ƒëi·ªÉm F?"

### 3. Ambiguous Queries

Test queries c√≥ nhi·ªÅu √Ω nghƒ©a:
```json
{
  "question": "Th√¥i h·ªçc",
  "relevant_doc_ids": ["voluntary_withdrawal", "forced_withdrawal"],
  "department_id": 3
}
```

### 4. Regular Updates

- Th√™m test cases m·ªõi khi c√≥ user feedback
- Update relevant IDs khi data thay ƒë·ªïi
- Run evaluation after m·ªói major update

## Integration with CI/CD

### 1. Add to Test Suite

```python
# tests/test_rag_evaluation.py
import pytest
from app.rag.evaluation import RAGEvaluator

@pytest.mark.asyncio
async def test_rag_quality():
    evaluator = RAGEvaluator()
    evaluator.load_test_cases("tests/evaluation/test_cases.json")
    
    metrics = await evaluator.evaluate_retrieval(k=5)
    
    # Set quality thresholds
    assert metrics.precision_at_k >= 0.60, "Precision@5 too low"
    assert metrics.recall_at_k >= 0.70, "Recall@5 too low"
    assert metrics.hit_rate >= 0.90, "Hit rate too low"
```

### 2. GitHub Actions

```yaml
# .github/workflows/rag-evaluation.yml
name: RAG Evaluation

on: [push, pull_request]

jobs:
  evaluate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run RAG Evaluation
        run: |
          cd apps/api
          python scripts/run_evaluation.py
      - name: Upload Results
        uses: actions/upload-artifact@v3
        with:
          name: evaluation-results
          path: apps/api/tests/evaluation/results.json
```

## Confidence Benchmarking Pipeline

### 1. Prepare Expected Answers

- Add curated test cases with `expected_answer` to `apps/api/test-case.json`.
- Each entry should contain `question`, `relevant_doc_ids`, and the canonical fact we expect the model to state.

```json
{
  "question": "H·ªçc ph√≠ nƒÉm h·ªçc 2024 l√† bao nhi√™u?",
  "expected_answer": "H·ªçc ph√≠ nƒÉm h·ªçc 2024-2025 dao ƒë·ªông 150-180 tri·ªáu VNƒê m·ªói nƒÉm t√πy ng√†nh.",
  "relevant_doc_ids": ["tuition_2024"],
  "department_id": 1,
  "language": "vi"
}
```

### 2. Run the Benchmark

```bash
cd apps/api
python scripts/run_confidence_benchmark.py \
  --cases test-case.json \
  --output tests/evaluation/confidence_report.json
```

The script queries the RAG orchestrator, compares answers against `expected_answer`, and reports:

- **Accuracy & Coverage** of the answers judged correct.
- **Brier score / Expected calibration error** for the confidence field.
- **Recommended confidence threshold** (maximizes F1 between precision/recall).
- **Calibration buckets** showing how accuracy aligns with confidence ranges.

Use these metrics to tune `CONFIDENCE_THRESHOLD`, monitor regressions, or gate deployments.

## Troubleshooting

### Low Precision

- Check if retrieval is returning irrelevant results
- Tune hybrid search weights (alpha)
- Improve embeddings quality

### Low Recall

- Increase `top_k` parameter
- Check if relevant docs are in database
- Improve query expansion

### High Fallback Rate

- Check LLM prompt quality
- Verify context is sufficient
- Review confidence thresholds

### High Latency

- Enable caching for embeddings
- Optimize database queries
- Consider async processing

## Future Enhancements

1. **Answer Quality Metrics**
   - BLEU/ROUGE scores v·ªõi expected_answer
   - Human evaluation interface
   - A/B testing framework

2. **Automated Test Generation**
   - Generate test cases from user logs
   - Active learning for hard cases

3. **Real-time Monitoring**
   - Dashboard v·ªõi Grafana/Kibana
   - Alerts khi metrics drop
   - Trend analysis over time
