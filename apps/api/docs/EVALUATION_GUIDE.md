# RAG Evaluation Framework

## Overview

Framework Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng RAG pipeline vá»›i 2 nhÃ³m metrics:
- **Retrieval Metrics**: Precision@K, Recall@K, F1@K, MRR, NDCG, Hit Rate
- **Answer Quality Metrics**: Confidence, Fallback Rate, Latency, Number of Sources

## Quick Start

### 1. Táº¡o Test Cases

Táº¡o file `tests/evaluation/test_cases.json`:

```json
[
  {
    "question": "Há»c phÃ­ nÄƒm há»c 2024 lÃ  bao nhiÃªu?",
    "relevant_doc_ids": ["tuition_2024"],
    "category": "tuition",
    "language": "vi"
  },
  {
    "question": "CNTT",
    "relevant_doc_ids": ["program_cntt"],
    "category": "program",
    "language": "vi"
  }
]
```

### 2. Run Evaluation

```bash
cd apps/api
python scripts/run_evaluation.py
```

### 3. Check Results

Káº¿t quáº£ Ä‘Æ°á»£c lÆ°u vÃ o `tests/evaluation/results.json` vÃ  in ra console:

```
======================================================================
RAG EVALUATION REPORT
======================================================================

Total Test Cases: 10
Top-K: 5

ðŸ“Š RETRIEVAL METRICS:
----------------------------------------------------------------------
  Precision@5: 0.650
  Recall@5:    0.820
  F1@5:         0.725
  MRR:            0.780
  NDCG:           0.845
  Hit Rate:       95.00%

ðŸ¤– ANSWER QUALITY METRICS:
----------------------------------------------------------------------
  Avg Confidence: 0.742
  Fallback Rate:  8.00%
  Avg Latency:    1250ms
  Avg Sources:    4.2

======================================================================
```

## Metrics Explained

### Retrieval Metrics

**Precision@K**: Tá»‰ lá»‡ káº¿t quáº£ relevant trong top-K
- `Precision@5 = 0.650` â†’ 65% káº¿t quáº£ trong top-5 lÃ  relevant

**Recall@K**: Tá»‰ lá»‡ relevant documents Ä‘Æ°á»£c tÃ¬m tháº¥y trong top-K
- `Recall@5 = 0.820` â†’ TÃ¬m Ä‘Æ°á»£c 82% relevant documents

**F1@K**: Harmonic mean cá»§a Precision vÃ  Recall
- `F1@5 = 0.725` â†’ Balance giá»¯a precision vÃ  recall

**MRR (Mean Reciprocal Rank)**: Trung bÃ¬nh nghá»‹ch Ä‘áº£o cá»§a rank Ä‘áº§u tiÃªn cÃ³ relevant result
- `MRR = 0.780` â†’ Trung bÃ¬nh relevant result Ä‘áº§u tiÃªn á»Ÿ vá»‹ trÃ­ 1.28

**NDCG (Normalized Discounted Cumulative Gain)**: ÄÃ¡nh giÃ¡ ranking quality
- `NDCG = 0.845` â†’ Ranking quality cao (1.0 lÃ  perfect)

**Hit Rate**: % queries cÃ³ Ã­t nháº¥t 1 relevant result trong top-K
- `Hit Rate = 95%` â†’ 95% queries tÃ¬m Ä‘Æ°á»£c Ã­t nháº¥t 1 relevant result

### Answer Quality Metrics

**Avg Confidence**: Trung bÃ¬nh confidence score cá»§a answers
- `0.742` â†’ Há»‡ thá»‘ng khÃ¡ tá»± tin vá» cÃ¢u tráº£ lá»i

**Fallback Rate**: Tá»‰ lá»‡ queries trigger fallback
- `8%` â†’ 8% queries khÃ´ng Ä‘á»§ context Ä‘á»ƒ tráº£ lá»i

**Avg Latency**: Thá»i gian trung bÃ¬nh Ä‘á»ƒ generate answer
- `1250ms` â†’ Trung bÃ¬nh 1.25 giÃ¢y/query

**Avg Sources**: Sá»‘ lÆ°á»£ng sources trung bÃ¬nh Ä‘Æ°á»£c sá»­ dá»¥ng
- `4.2` â†’ Trung bÃ¬nh sá»­ dá»¥ng 4.2 sources/answer

## Test Cases Format

### Basic Format

```json
{
  "question": "CÃ¢u há»i test",
  "relevant_doc_ids": ["doc_id_1", "doc_id_2"],
  "category": "tuition",
  "language": "vi"
}
```

### Advanced Format (vá»›i chunk-level evaluation)

```json
{
  "question": "Há»c phÃ­ lÃ  bao nhiÃªu?",
  "relevant_doc_ids": ["tuition_2024"],
  "relevant_chunk_ids": ["chunk_abc", "chunk_def"],
  "expected_answer": "Há»c phÃ­ nÄƒm 2024 lÃ  25 triá»‡u Ä‘á»“ng/nÄƒm",
  "category": "tuition",
  "language": "vi"
}
```

### Fields

- `question` (required): CÃ¢u há»i test
- `relevant_doc_ids` (required): IDs cá»§a documents relevant
- `relevant_chunk_ids` (optional): IDs cá»§a chunks relevant (chi tiáº¿t hÆ¡n doc-level)
- `expected_answer` (optional): CÃ¢u tráº£ lá»i mong Ä‘á»£i (Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ answer quality)
- `category` (optional): Category cá»§a cÃ¢u há»i (tuition, program, policy, etc.)
- `language` (optional): NgÃ´n ngá»¯ (vi, en)

## How to Get Document/Chunk IDs

### Option 1: From Database

```python
from app.repositories.faq_repository import FAQRepository

repo = FAQRepository()
faqs = await repo.get_all()

for faq in faqs:
    print(f"Question: {faq.question}")
    print(f"Doc ID: {faq.id}")
```

### Option 2: From ChromaDB

```python
from app.rag.retriever import Retriever

retriever = Retriever()
results = retriever.retrieve("há»c phÃ­", top_k=5)

for result in results:
    print(f"Chunk ID: {result.get('chunk_id')}")
    print(f"Doc ID: {result.get('document_id')}")
    print(f"Content: {result.get('content')[:100]}...")
```

### Option 3: Manual Labeling

1. Query system vá»›i test question
2. Xem sources Ä‘Æ°á»£c tráº£ vá»
3. Note láº¡i IDs cá»§a sources relevant
4. Add vÃ o test cases

## Best Practices

### 1. Diverse Test Cases

Cover nhiá»u categories:
```json
[
  {"category": "tuition", "question": "Há»c phÃ­..."},
  {"category": "program", "question": "CNTT..."},
  {"category": "policy", "question": "ThÃ´i há»c..."},
  {"category": "scholarship", "question": "Há»c bá»•ng..."}
]
```

### 2. Query Complexity Levels

- **Short queries**: "CNTT", "Há»c phÃ­"
- **Medium queries**: "Há»c phÃ­ nÄƒm 2024"
- **Long queries**: "LÃ m tháº¿ nÃ o Ä‘á»ƒ Ä‘Äƒng kÃ½ há»c láº¡i mÃ´n bá»‹ Ä‘iá»ƒm F?"

### 3. Ambiguous Queries

Test queries cÃ³ nhiá»u Ã½ nghÄ©a:
```json
{
  "question": "ThÃ´i há»c",
  "relevant_doc_ids": ["voluntary_withdrawal", "forced_withdrawal"],
  "category": "policy"
}
```

### 4. Regular Updates

- ThÃªm test cases má»›i khi cÃ³ user feedback
- Update relevant IDs khi data thay Ä‘á»•i
- Run evaluation after má»—i major update

## Integration with CI/CD

### 1. Add to Test Suite

```python
# tests/test_rag_evaluation.py
import pytest
from app.rag.evaluation import RAGEvaluator

@pytest.mark.asyncio
async def test_rag_quality():
    evaluator = RAGEvaluator()
    evaluator.load_test_cases("tests/evaluation/test_cases.json")
    
    metrics = await evaluator.evaluate_retrieval(k=5)
    
    # Set quality thresholds
    assert metrics.precision_at_k >= 0.60, "Precision@5 too low"
    assert metrics.recall_at_k >= 0.70, "Recall@5 too low"
    assert metrics.hit_rate >= 0.90, "Hit rate too low"
```

### 2. GitHub Actions

```yaml
# .github/workflows/rag-evaluation.yml
name: RAG Evaluation

on: [push, pull_request]

jobs:
  evaluate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run RAG Evaluation
        run: |
          cd apps/api
          python scripts/run_evaluation.py
      - name: Upload Results
        uses: actions/upload-artifact@v3
        with:
          name: evaluation-results
          path: apps/api/tests/evaluation/results.json
```

## Troubleshooting

### Low Precision

- Check if retrieval is returning irrelevant results
- Tune hybrid search weights (alpha)
- Improve embeddings quality

### Low Recall

- Increase `top_k` parameter
- Check if relevant docs are in database
- Improve query expansion

### High Fallback Rate

- Check LLM prompt quality
- Verify context is sufficient
- Review confidence thresholds

### High Latency

- Enable caching for embeddings
- Optimize database queries
- Consider async processing

## Future Enhancements

1. **Answer Quality Metrics**
   - BLEU/ROUGE scores vá»›i expected_answer
   - Human evaluation interface
   - A/B testing framework

2. **Automated Test Generation**
   - Generate test cases from user logs
   - Active learning for hard cases

3. **Real-time Monitoring**
   - Dashboard vá»›i Grafana/Kibana
   - Alerts khi metrics drop
   - Trend analysis over time
